\documentclass{article}


\title{Differentiation}
\author{Lili Zhao}
\input{head/head.tex}


\begin{document}
\maketitle
%========================================
%########################################
%----------------------------------------


For any $f: D_f \subseteq X \to Y$, let $X$ be a normed vector space over $\mathbb K_X$ as the scalar field, and let $Y$ be a normed vector space over $\mathbb K_Y$ as the scalar field.

Also, for any normed vector space $X$, denote $\| \cdot \|_X$ for norm function of $X$.


\section{Differentiable Mappings}
%========================================

\begin{definition}
	\label{def: differentiable mappings}
	A mapping $f: D_f \subseteq X \to Y$ is said to be \textit{differentiable}
	 at $\mathbf p$ iff there exists a linear mapping $\phi: X \to Y$, such that
	$$
	\phi(\mathbf u) \sim f(\mathbf p + \mathbf u) - f(\mathbf p) \quad : \mathbf u \to \mathbf p.
	$$
\end{definition}


\begin{proposition}
	$f$ is differentiable at a point $\mathbf p \in D_f$ iff there exists a linear mapping $\phi: X \to Y$, such that
	$$
	\lim_{\mathbf u \to \mathbf 0_X} \frac{f(\mathbf p + \mathbf u) - f(\mathbf p) - \phi(\mathbf u)}{\| \mathbf u \|_X} = \mathbf 0_Y.
	$$
	\begin{proof}
		As $f$ is differentiable at $\mathbf p$, we have
		$$
		\phi()
		$$
	\end{proof}
\end{proposition}


\begin{proposition}
	If $f$ is differentiable, then $\phi$ in Definition \ref{def: differentiable mappings} is unique.
	
	\begin{proof}
		Suppose there exists a linear mapping $\lambda: X\to Y$ such that
		$$
		\lim_{\mathbf u \to 0_X} \frac{f(\mathbf p + \mathbf u) - f(\mathbf p) - \lambda(\mathbf u)}{\| \mathbf u \|_X} = \mathbf 0_Y.
		$$
		
		It is easy to obtain that
		$$
		\begin{aligned}
			\phi(\mathbf{\hat u}) = \lambda(\mathbf{\hat u}),
		\end{aligned}
		$$
		which implies $\phi = \lambda$.
	\end{proof}
\end{proposition}


\begin{proposition}
	If $f$ is differentiable at $\mathbf p$, then $f$ is continuous at $\mathbf p$.
	
	\begin{proof}
		Assume $f$ is differentiable at $\mathbf p$, then there exists a linear mapping $\phi: X \to Y$ such that
		$$
		\lim_{\mathbf u \to \mathbf 0_X} \frac{f(\mathbf p + \mathbf u) - f(\mathbf p)}{\| \mathbf u \|_X} = \phi(\mathbf{\hat u}).
		$$
		
		Thus, for any $\varepsilon > 0$, there exists $\delta > 0$, such that for any $\mathbf x \in B_X(\mathbf 0_X, \delta) \setminus \{\mathbf 0_X\}$,
		$$
		\begin{aligned}
			\frac{f(\mathbf p + \mathbf u) - f(\mathbf p)}{\| \mathbf u \|_X} \in B_Y(\phi(\mathbf u), \varepsilon) &\iff f(\mathbf p + \mathbf u) - f(\mathbf p) \in B_Y(\phi(\mathbf{\hat u}), \varepsilon \| \mathbf u \|_X),
		\end{aligned}
		$$
		
		Note that, as $\mathbf u \to \mathbf 0_X$, we only consider $\| \mathbf u \|_X < 0$. So, $\varepsilon \| u \|_X < \varepsilon$.
		
		Thus, we have
		$$
		\begin{aligned}
			& \lim_{\mathbf u \to \mathbf 0_X} [f(\mathbf p + \mathbf u) - f(\mathbf p)] = \mathbf 0_Y = f(\mathbf p + \mathbf 0_X) - f(\mathbf p) \\
			\iff & \lim_{\mathbf u \to \mathbf 0_X} f(\mathbf p + \mathbf u) - f(\mathbf p) = f(\mathbf p) - f(\mathbf p) \\
			\iff & \lim_{\mathbf u \to \mathbf 0_X} f(\mathbf p + \mathbf u) = f(\mathbf p),
		\end{aligned}
		$$
		which implies that $f$ is continuous at $\mathbf p$.
	\end{proof}
\end{proposition}


\section{Asymptotic Notation}
%========================================



\begin{definition}
	Let $f: D_f \subseteq X \to Y$, and let $\mathbf p \in D_f$. Assume
	$$
	\lim_{\mathbf x \to \mathbf p} f(\mathbf x) \in Y.
	$$
	
	The \textit{little-o} of $f$ as $\mathbf x \to \mathbf p$ is a set
	$$
	o(f(\mathbf x)) = \left\{ g: D_g \subseteq X \to Y_g : \lim_{\mathbf x \to \mathbf p} \frac{g(\mathbf x)}{\|f(\mathbf x)\|_Y} = \mathbf 0_{Y_g} \right\}, \text{ as $\mathbf x \to \mathbf p$},
	$$
	where for any mapping $g$, $Y_g$ is a normed vector space over $\mathbb K_{Y_g}$.
	
	Equivalently, for any mapping $g$ defined on $X$, $g \in o(f(\mathbf p))$ iff for any $\varepsilon \in \mathbb R_{> 0}$, there exists a neighbourhood $N$ of $\mathbf p$, such that for any $\mathbf x \in N$,
	$$
	\left\| \frac{g(\mathbf x)}{\| f(\mathbf x) \|_Y} \right\|_{Y_g} < \varepsilon,
	$$
	or, equivalently,
	$$
	\frac{g(\mathbf x)}{\| f(\mathbf x) \|_Y} \in B(\mathbf 0_{Y_g}, \varepsilon).
	$$
\end{definition}


\begin{proposition}
	A mapping $f: D_f \subseteq X \to Y$ is differentiable at a point $\mathbf p \in D_f$ iff there exists a linear mapping $\phi: X \to Y$, such that
	$$
	f(\mathbf p + \mathbf h) - f(\mathbf p) - \phi(\mathbf h) = o(\mathbf h)
	$$
	as $\mathbf h \to \mathbf 0_X$.
	
	\begin{proof}
		$f$ is differentiable at $\mathbf p$ iff there exists an $\alpha : D_\alpha \subseteq X \to Y$ with $\alpha(\mathbf x) \to \mathbf 0_Y$ as $\mathbf x \to \mathbf 0_X$, such that
		$$
		\begin{aligned}
			& \lim_{\mathbf h \to \mathbf 0_X} \frac{f(\mathbf p + \mathbf h) - f(\mathbf p) - \phi(\mathbf h)}{\| \mathbf h \|_X} = \lim_{\mathbf h \to \mathbf 0_X} \alpha(\mathbf h) \\
			\iff & \lim_{\mathbf h \to \mathbf 0_X} \frac{f(\mathbf p + \mathbf h) - f(\mathbf p) - \phi(\mathbf h)}{\| \mathbf h \|_X} = \lim_{\mathbf h \to \mathbf 0_X} \frac{\alpha (\mathbf h) \| \mathbf h \|_X}{\| \mathbf h \|_X} \\
			\iff & \lim_{\mathbf h \to \mathbf 0_X} \frac{f(\mathbf p + \mathbf h) - f(\mathbf p) - \phi(\mathbf h)}{\| \mathbf h \|_X} = \lim_{\mathbf h \to \mathbf 0_X} \frac{o(\mathbf h)}{\| \mathbf h \|_X}. \\
		\end{aligned}
		$$
		
		The equation holds iff for any $\varepsilon > 0$, there exists $\delta > 0$, such that for any $\mathbf h \in B(\mathbf 0_X, \delta) \setminus \{\mathbf 0_X\}$,
		$$
		\begin{aligned}
			& f(\mathbf p + \mathbf h) - f(\mathbf p) - \phi (\mathbf h) - o(\mathbf h) \in B(\mathbf 0_Y, \varepsilon) \\
			\iff & f(\mathbf p + \mathbf h) - f(\mathbf p) - \phi(\mathbf h) \in B(o(\mathbf h), \varepsilon).
		\end{aligned}
		$$
		This holds iff
		$$
		f(\mathbf p + \mathbf h) - f(\mathbf p) - \phi(\mathbf h) = o(\mathbf h),
		$$
		as $\mathbf h \to \mathbf 0_X$.
	\end{proof}
\end{proposition}


\begin{definition}
	The \textit{big-O} of $f$ as $\mathbf x \to \mathbf p$ is a set
	$$
	O(f(\mathbf x)) = \left\{
		g: D_g \subseteq X \to Y_g : \lim_{\mathbf x \to \mathbf p} \frac{g(\mathbf x)}{\| f(\mathbf x) \|_Y} \in Y_g
	\right\}, \text{ as $\mathbf x \to \mathbf p$},
	$$
	where for any mapping $g$, $Y_g$ is a normed vector space over $\mathbb K_{Y_g}$.
	
	Equivalently, for any mapping $g$ defined on $X$, $g \in O(f(\mathbf p))$ iff there exists an $\varepsilon \in \mathbb R_{> 0}$, such that.........
\end{definition}



\begin{proposition}
	A mapping $f: D_f \subseteq X \to Y$ is differentiable at a point $\mathbf p \in D_f$ iff there exists a linear mapping $\phi: X \to Y$, such that
	$$
	f(\mathbf p + \mathbf h) \sim l(\mathbf h), \text{ as $\mathbf h \to \mathbf 0_{X}$},
	$$
	where
	$$
	l(\mathbf h) = f(\mathbf p) + \phi(\mathbf h).
	$$
	
	\begin{proof}
		$$
		f(\mathbf p + \mathbf h) = f(\mathbf p) + \phi(\mathbf h) + o(\mathbf h).
		$$
	\end{proof}
\end{proposition}



\section{Directional Derivatives}
%========================================

\begin{definition}
	Let $f: D_f \subseteq X \to Y$.
	Let $\mathbf u \in X$ and let $\mathbf p \in D_f$.
	
	The \textit{$\mathbf u$-directional derivative} of $f$ at $\mathbf p$ is defined as
	$$
	\nabla_{\mathbf u} f(\mathbf p) := \lim_{t \to 0} \frac{f(\mathbf p + t\mathbf u) - f(\mathbf u)}{t},
	$$
	if the limit exists in $Y$.
\end{definition}


\begin{proposition}
	If $\nabla_{\mathbf u} f(\mathbf p) \in Y$, then
	$$
	\nabla_{\mathbf u} f(\mathbf p) = \left. \frac{df(r(t))}{dt} \right|_{t = 0},
	$$
	where $r:\mathbb R \to X: t \mapsto \mathbf p + t\mathbf u$.
	
	\begin{proof}
		Let $r: \mathbb R \to X$ be defined as
		$$
		r(t) := \mathbf p + t \mathbf u.
		$$
		
		Let, $h = f \circ r$.
		$$
		\nabla_{\mathbf u} f(\mathbf p) = \lim_{t \to 0} \frac{h(t) - h(0)}{t} = \frac{dh}{dt}(0).
		$$
	\end{proof}
\end{proposition}


\begin{proposition}
	\label{prop: differentiable implies continuous}
	If $f$ is differentiable at $p$, then $\nabla_{\mathbf u} f(\mathbf p) \in Y$ for any $\mathbf u \in X$.
	
	In particular, the linear mapping $\phi$ in Definition \ref{def: differentiable mappings} is defined as
	$$
	\phi(\mathbf x) := \nabla_{\mathbf x} f(\mathbf u).
	$$
	
	\begin{proof}
		Assume $f$ is differentiable at $\mathbf p$, then there exists a linear mapping $\phi: X \to Y$ such that
		$$
		\lim_{\mathbf u \to \mathbf 0_X} \frac{f(\mathbf p + \mathbf u) - f(\mathbf p)}{\|\mathbf u\|_{X}} = \phi\left( \frac{\mathbf u}{\| \mathbf u \|_X} \right).
		$$
		
		Let $\mathbf u = t \mathbf w$ where $t \in \mathbb R$ and $\mathbf w \in X$. Then, we have
		$$
		\begin{aligned}
			&\lim_{t \to 0} \frac{f(\mathbf p + t\mathbf w) - f(\mathbf p)}{|t|\| \mathbf u \|_X} = \phi\left( \frac{t}{|t|} \mathbf w \right) \\
			\iff & \lim_{t \to 0} \frac{f(\mathbf p + t\mathbf w) - f(\mathbf p)}{t} = \phi(\mathbf w) = \nabla_{\mathbf w} f(\mathbf p).
		\end{aligned}
		$$
	\end{proof}
\end{proposition}


\begin{proposition}
	If $\nabla_{\mathbf u} f(\mathbf p) \in Y$ for a given $\mathbf u \in X \setminus \{\mathbf 0_X\}$, then
	$$
	s \nabla_{\mathbf u} f(\mathbf p) = \nabla_{s\mathbf u} f(\mathbf p),
	$$
	for any $s \in \mathbb R$.
	
	If $f$ is differentiable at $\mathbf p$, then, for any $\mathbf u, \mathbf v \in X$.
	$$
	\nabla_{\mathbf u + \mathbf v} f(\mathbf p) = \nabla_{\mathbf u} f(\mathbf p) + \nabla_{\mathbf v} f(\mathbf p).
	$$
	
	\begin{proof}
		Let $t = s\theta$, then we have
		$$
		\begin{aligned}
			s\nabla_{\mathbf u} f(\mathbf p) &= s \lim_{t \to 0} \frac{f(\mathbf p + t\mathbf u) - f(\mathbf p)}{t} \\
			&= \lim_{\theta \to 0} \frac{f(\mathbf p + s\theta \mathbf u) - f(\mathbf p)}{\theta} \\
			&= \nabla_{s\mathbf u} f(\mathbf p).
		\end{aligned}
		$$
		
		Assume $f$ is differentiable at $\mathbf p$. Let $\phi:X \to Y$ be defined as
		$$
		\phi(\mathbf x) := \nabla_{\mathbf x} f(\mathbf p).
		$$
		
		By Proposition \ref{prop: differentiable implies continuous}, $\phi$ is linear, so for any $u, v \in X$,
		$$
		\nabla_{\mathbf u + \mathbf v}f(\mathbf p) = \phi(\mathbf u + \mathbf v) = \phi(\mathbf u) + \phi(\mathbf v) = \nabla_{\mathbf u} f(\mathbf p) + \nabla_{\mathbf v} f(\mathbf p).
		$$
	\end{proof}
\end{proposition}


\begin{example}
	However, $f$ is not necessarily differentiable at $\mathbf p$, even if $\nabla_{\mathbf u} f(\mathbf p) \in Y$ for any $\mathbf u \in X \setminus \{\mathbf 0_X\}$.
	
	Let $f: \mathbb R^2 \to \mathbb R$ be defined as
	$$
	f(x,y) :=
	\begin{cases}
		0 & : x\ne y, \\
		x+ y & : x= y.
	\end{cases}
	$$
	
	For any $(u_x, u_y) \in \mathbb R^2$, $f(u_x, u_y) \in \mathbb R$.
	
	Let $\varphi: \mathbb R^2 \to \mathbb R$ be defined as
	$$
	\varphi(x,y) := \nabla_{(x,y)} f(0,0).
	$$
	
	If $f$ is differentiable, $\varphi$ should be linear. But,
	$$
	2 = \varphi(1,1) \ne \varphi(1,0) + \varphi(0,1) = 0.
	$$
	Thus, $\varphi$ is not linear.
\end{example}


\section{Partial Derivatives}
%========================================


\begin{definition}
	Let $f: D_f \subseteq X \to Y$, where $X = \mathbb K^n$. Let $\mathbf p \in X$
	
	The \textit{partial derivative} of $f$ at $\mathbf p$ respect to $x_i$ ($i \in \{1, \ldots, n\}$) is defined as the $\mathbf{\hat e}_i$-directional derivative of $f$ at $\mathbf p$; i.e.,
	$$
	\frac{\partial f}{\partial x_i}(\mathbf p) := \nabla_{\mathbf{\hat e}_i} f(\mathbf p) = \lim_{t \to 0} \frac{f(\mathbf p + t \mathbf{\hat e}_i) - f(\mathbf p)}{t}.
	$$
\end{definition}


\begin{proposition}[symmetry of second derivatives]
	If $\displaystyle \frac{\partial f}{\partial x_i \partial x_k}$ and $\displaystyle \frac{\partial f}{\partial x_k \partial x_i}$ both exist in $Y$ for some $i,j \in \{1, \ldots, n\}$, then	
	
	$$
	\frac{\partial^2 f}{\partial x_i \partial x_k}(\mathbf p) = \frac{\partial^2 f}{\partial x_k \partial x_i}.
	$$
	
	\begin{proof}
		\begin{equation}
			\tag{i}
			\begin{aligned}
				\frac{\partial^2 f}{\partial x_i \partial x_k}(\mathbf p) &= \frac{\partial}{\partial x_i} \frac{\partial f}{\partial x_k} (\mathbf p)
				= \lim_{t \to 0} \frac{\frac{\partial f}{\partial x_k} (\mathbf p + t \mathbf{\hat e}_i) - \frac{\partial f}{\partial x_k} (\mathbf p)}{t}.
			\end{aligned}
		\end{equation}
		
		Consider
		\begin{equation}
			\tag{ii}
			\begin{aligned}
				& \frac{\partial f}{\partial x_k} (\mathbf p + t \mathbf{\hat e}_i) - \frac{\partial f}{\partial x_k} (\mathbf p) \\
				=& \lim_{t \to 0} \frac{f(\mathbf p + t \mathbf{\hat e}_i + t \mathbf{\hat e}_k) - f(\mathbf p + t \mathbf{\hat e}_i)}{t} - \lim_{t \to 0} \frac{f(\mathbf p + t\mathbf{\hat e}_k) - f(\mathbf p)}{t} \\
				=& \lim_{t \to 0} \frac{f(\mathbf p + t \mathbf{\hat e}_k + t \mathbf{\hat e}_i) - f(\mathbf p + t\mathbf{\hat e}_k)}{t} - \lim_{t \to 0} \frac{f(\mathbf p + t\mathbf{\hat e}_i) - f(\mathbf p)}{t} \\
				=& \frac{\partial f}{\partial x_i} (\mathbf p + t\mathbf{\hat e}_k) - \frac{\partial f}{\partial x_i} (\mathbf p).
			\end{aligned}
		\end{equation}
		
		Substitute (ii) into (i), we have
		$$
		\begin{aligned}
			\frac{\partial^2 f}{\partial x_i \partial x_k}(\mathbf p) &= \lim_{t \to 0} \frac{\frac{\partial f}{\partial x_i}f(\mathbf p + t \mathbf{\hat e}_k) - \frac{\partial f}{\partial x_i}f(\mathbf p)}{t} \\
			&= \frac{\partial}{\partial x_k} \frac{\partial f}{\partial x_i} (\mathbf p) \\
			&= \frac{\partial^2 f}{\partial x_k \partial x_i}.
		\end{aligned}
		$$
		
	\end{proof}
\end{proposition}


\section{Jacobian Matrices and Gradient}
%========================================


\begin{definition}
	Let $f: D_f \subseteq X \to Y$, where $X = \mathbb K_X^n$.
		
	The \textit{Jacobian matrix} of $f$ at $\mathbf p$ is defined as
	$$
	\mathbf J_{f}(\mathbf p) :=
	\left[
	\begin{matrix}
		\displaystyle \frac{\partial f}{\partial x_1}(\mathbf p) &\ldots & \displaystyle \frac{\partial f}{\partial x_n} (\mathbf p)
	\end{matrix}
	\right].
	$$
\end{definition}


\begin{note}
	Customarily, if $Y = \mathbb K_Y$, we call $\mathbf J_f^\top (\mathbf p)$ the \textit{gradient} of $f$ at $\mathbf p$, and denote $\nabla f(\mathbf p)$ for $\mathbf J_f^{\top} (\mathbf p)$. If $Y = \mathbb K_Y^m$, then
	$$
	\mathbf J_{f}(\mathbf p) =
	\left[
	\begin{matrix}
		\nabla^\top f_1(\mathbf p) \\
		\vdots \\
		\nabla^\top f_m(\mathbf p)
	\end{matrix}
	\right].
	$$
\end{note}


\begin{proposition}
	Let $f: D_f \subseteq X \to Y$, where $X = \mathbb K_X^n$. If $f$ is differentiable at $\mathbf p$, then we have.
	
	$$
	\nabla_{\mathbf u} f(\mathbf p) = \mathbf J_{f}(\mathbf p) \; \mathbf u.
	$$
\end{proposition}


\begin{proposition}
	\label{prop: chain rule of partial derivatives}
	Let $X = \mathbb K_X^m$, $Y = \mathbb K_Y^n$.
	
	Let $f: D_f \subseteq X \to Y$ be differentiable at $\mathbf p$.
	
	Let $g: D_g \subseteq Y \to Z$ be differentiable at $f(\mathbf p)$.
	
	For any $i \in \{1, \ldots , m\}$, we have
	$$
	\frac{\partial (g \circ f)}{\partial x_i }(\mathbf p) = \mathbf J_{g} (f(\mathbf p)) \frac{\partial f}{\partial x_i} (\mathbf p).
	$$
	
	\begin{proof}
	
		Define $\varphi(t) := f(\mathbf p + t \mathbf{\hat e}_i)$, where $\mathbf{\hat e}_i$ denotes the $i$-th basis of $X$. Then,
		$$
		\frac{\partial (g \circ f)}{\partial x_i}(\mathbf p) = \lim_{t \to 0} \frac{g(\varphi(t)) - g(\varphi(0))}{t}.
		$$
		
		Assume there exists neighbourhood $N$ of $\mathbf p$ such that $f \restriction_N$ is constant, then the limit above is zero, and $\frac{\partial f}{\partial x_i}(\mathbf p)$ is also zero. There is nothing to prove in this case. So, assume that for any neighbourhood $N$ of $\mathbf p$, $f \restriction_N$ is not constant.
		
		As $f$ is differentiable at $\mathbf p$, $\frac{\partial f}{\partial x_i} (\mathbf p) \in Y$, and $\varphi'(0) \in Y$. (This is also a chain rule, but why?) So, we can define
		$$
		\lambda (t) := t \varphi'(0) + \varphi(0).
		$$
		
		As
		$$
		\lim_{h \to 0}\frac{\varphi (h)}{\| \lambda(h) \|_Y} = \mathbf 1_Y = \lim_{h \to 0}\frac{\lambda (h)}{\| \varphi(h) \|_Y},
		$$
		
		we have $\varphi \to \lambda$ as $h \to 0$. Thus, we have
		$$
		\begin{aligned}
			\frac{\partial (g \circ f)}{\partial x_i} &= \lim_{t \to 0} \frac{g(\varphi(0) + t\varphi'(0)) - g(\varphi(0))}{t} \\
			&= \nabla_{\varphi'(0)} g (\varphi(0)) \\
			&= \mathbf J_g(\varphi(0)) \; \varphi'(0) \\
			&= \mathbf J_{g}(f(\mathbf p)) \; \frac{\partial f}{\partial x_i}(\mathbf p).
		\end{aligned}
		$$
	\end{proof}
\end{proposition}


\begin{proposition}	
	\label{prop: chain rule of jacobian}
	With the conditions above, we have
	$$
	\mathbf J_{g \circ f}(\mathbf p) = \mathbf J_{g}(f(\mathbf p)) \; \mathbf J_{f}(\mathbf p).
	$$
	
	\begin{proof}
		First, consider $\mathbf J_{g \circ f}(\mathbf p)$ as an $1 \times m$ matrix:
		$$
		\mathbf J_{g \circ f}(\mathbf p) =
		\left[
		\begin{matrix}
			\displaystyle \frac{\partial (g \circ f)}{\partial x_1}(\mathbf p) & \cdots & \displaystyle \frac{\partial (g \circ f)}{\partial x_m}(\mathbf p) \\
		\end{matrix}
		\right].
		$$
		
		For any $i \in \{1, \ldots, m\}$, we have
		$$
		\frac{\partial (g \circ f)}{\partial x_i}(\mathbf p) = \mathbf J_{g}(f(\mathbf p)) \; \frac{\partial f}{\partial x_i}(\mathbf p).
		$$
		
		Then we have
		$$
		\begin{aligned}
			\mathbf J_{g \circ f}(\mathbf p) &= \mathbf J_g (f(\mathbf p))
			\left[
			\begin{matrix}
				\displaystyle \frac{\partial f}{\partial x_1}(\mathbf p) & \cdots & \displaystyle \frac{\partial f}{\partial x_m}(\mathbf p)
			\end{matrix}
			\right]
			= \mathbf J_g (f(\mathbf p)) \; \mathbf J_f(\mathbf p).
		\end{aligned}
		$$ 
		
		The proof is done.
	\end{proof}
\end{proposition}


\begin{note}
	In this proof, if $Z = \mathbb K_Z^r$, then,
	$$
	\begin{aligned}
	\mathbf J_{g \circ f} (\mathbf p) &=
	\left[
	\begin{matrix}
		\displaystyle \nabla^\top g_1(f(\mathbf p)) \ \frac{\partial f}{\partial x_1}(\mathbf p) & \cdots & \displaystyle \nabla^\top g_1(f(\mathbf p)) \ \frac{\partial f}{\partial x_m}(\mathbf p) \\
		\vdots & \ddots & \vdots \\
		\displaystyle \nabla^\top g_r(f(\mathbf p)) \ \frac{\partial f}{\partial x_1}(\mathbf p) & \cdots & \displaystyle \nabla^\top g_r(f(\mathbf p)) \ \frac{\partial f}{\partial x_m}(\mathbf p)
	\end{matrix}
	\right] \\
	&=
	\left[
	\begin{matrix}
		\nabla^\top g_1 (f(\mathbf p)) \\
		\vdots \\
		\nabla^\top g_r (f(\mathbf p))
	\end{matrix}
	\right]
	\left[
	\begin{matrix}
		\displaystyle\frac{\partial f}{\partial x_1} (\mathbf p) & \cdots & \displaystyle \frac{\partial f}{\partial x_m} (\mathbf p)
	\end{matrix}
	\right] \\
	&= \mathbf J_{g} (f(\mathbf p)) \ \mathbf J_f (\mathbf p).
	\end{aligned}
	$$
\end{note}


\begin{note}
	The notation $df(\mathbf p, \cdot)$ in \textit{Mathematical Analysis} by Elias Zakon can be considered as a mapping from $X \to Y$ be defined as
	$$
	df(\mathbf p, \mathbf x) := \nabla_{\mathbf x} f(\mathbf p) = \mathbf J_f(\mathbf p) \; \mathbf x.
	$$
	
	In this sense, we can consider Proposition \ref{prop: chain rule of jacobian} as
	$$
	\mathbf J_{g \circ f}(\mathbf p) = \mathbf J_{g}(f(\mathbf p)) \; \mathbf J_{f}(\mathbf p) = dg(f(\mathbf p), \cdot ) \circ df(\mathbf p, \cdot).
	$$
	
	Briefly ``The differential of the composite is the composite of differentials.''
\end{note}



\section{Taylor's Theorem}
%========================================



\begin{proposition}
	[Taylor's Theorem]
	\label{prop: taylor's theorem}
	Let $f: D_f \subseteq X \to Y$, where $X = \mathbb K_X^n$.
	
	If $f$ is $C^{k + 1}$ at $\mathbf p$, then, there exists an open set $U \subseteq D_f$ with $\mathbf p \in U$, such that for any $\mathbf u \in U$,
	$$
	f(\mathbf p + \mathbf u) = \sum_{i = 1}^k \frac{1}{i!} (\mathbf J \times \mathbf u)_f^i (\mathbf p) + R_k,
	$$
	where
	$$
	R_k = \frac{(\mathbf J \times \mathbf u)_f^{k + 1}(\mathbf c)}{(k + 1)!}
	$$
	for some $\mathbf c \in U$.
	
	\begin{proof}
		Let $\mathbf u \in U$, and let $s \in \mathbb K_X$ with $\mathbf u = t \mathbf{\hat u}$.
	
		Assume $f$ is $C^{1}$ at $\mathbf p$, then
		\begin{equation}
			\tag{1}
			\lim_{t \to 0} \frac{f(\mathbf p + \mathbf u) - f(\mathbf p) - \nabla_{\mathbf u}f(\mathbf p)}{t} = \lim_{t \to 0} \frac{t \alpha (t)}{t}
		\end{equation}
		for a mapping $\alpha: \mathbb K_X \to Y$ with
		$$
		\lim_{t \to 0} \alpha (t) = 0.
		$$
		
		Multiply both sides by $t$ (because little-o?)
		
		...
	\end{proof}
\end{proposition}






















%----------------------------------------
%########################################
%========================================
\end{document}